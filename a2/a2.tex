\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{listings} % For displaying code
\usepackage{algorithm2e} % pseudo-code

% Answers
\def\ans#1{\par\gre{Answer: #1}}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{a2f/#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{a2f/#2}\end{center}}
\newcommand{\matCode}[1]{\lstinputlisting[language=Matlab]{a2f/#1.m}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}


\begin{document}

\title{CPSC 340 Assignment 2 (due Friday October 13 ATE)}
\author{}
\date{}
\maketitle
\vspace{-4em}

Name: Hang Yee Wong\\
CS ID: r9i0b

Name: Joanne Chen\\
CS ID: r0a9

\section{Random Forests}

 
 \subsection{Implementation}

\begin{enumerate}
\item \texttt{decisionTree} and \texttt{randomTree} reach the base case when either the depth becomes 0 or when one of the subtrees is empty. Even when \emph{depth} is \emph{Inf}, our data is still finite so the training functions will reach an empty sub tree at some point, hence terminate.
\item  Because random tree have different splits with determined with randomly selected features, so it won't necessarily choose the 'optimal' split every time, so it may not reach 0 training error (i.e. it won't overfit). Therefore, the random tree model doesn't have a training error of 0.

  

\item \begin{verbatim}

function randomForest(X,y,depth,nTrees)
    (n,d) = size(X)
    subModels = Array{GenericModel}(nTrees)
    for m in 1:nTrees
        
    
    
    
    subModels[m]= randomTree(X,y,depth)
    end
 
	function predict(Xhat)
        a = length(subModels)
        (n,d) = size(Xhat)
        y = ones(n,a)

        for m in 1:length(subModels)
            y[:,m] = subModels[m].predict(Xhat)
        end
        
        yhat=zeros(n,1)
        for i in 1:n
            yhat[i] = mode(y[i,:])
        end

        return yhat
    end
    
    return GenericModel(predict)
end
\end{verbatim}

\item
Yes
 \begin{verbatim}
Train Error with depth-Inf decision tree: 0.000
Test Error with depth-Inf decision tree: 0.367
Train Error with depth-Inf random tree: 0.220
Test Error with depth-Inf random tree: 0.549
Train Error with depth-Inf random forest: 0.000
Test Error with depth-Inf random forest: 0.167
\end{verbatim}


\end{enumerate}

\subsection{Very-Short Answer Questions}

\begin{enumerate}
\item the runtime is too high
\item
    (b) Decrease the maximum depth of the trees in your forest.\\
    (d) Decrease the amount of data you consider for each tree (Use 0.8n objects instead of n).\\
    (f) Decrease the number of features you consider for each tree.\\
\item Translate some of the objects in various ways and add these to the training data set
 
\end{enumerate}


\section{K-Means Clustering}

\subsection{Selecting among k-means Initializations}
 
\begin{enumerate}
 \item
\begin{verbatim}
function kMeansError(X, y, W)
    error = 0;
    (n,d) = size(X);

    for i in 1:n
        error += sum((X[i,:] .- W[Int(y[i]),:]).^2);
    end

    return error;
end
\end{verbatim}

 \item The error decreases steadily until the last few iterations, where it remains the same.
 \item Minimum error obtained is 3071 (See Figure \ref{fig:q2_1_3})
\end{enumerate}
\vspace{5em}

  \begin{figure}
    \includegraphics[width=25em]{a2_q2_1_3.png}
    \caption{Clustering with minimum error at $k=4$ (Q2.1.3)}
    \label{fig:q2_1_3}
 \end{figure}

 \subsection{Selecting $k$ in k-means}

 \enum{
 \item The \emph{kMeansError} function computes the sum of the difference squared between the points and their means, but as we increase $k$, we will get more clusters, so there are more mean points, meaning the each point will become closer to its mean point. So basically this won't work because we're just averaging across less points as we increase $k$, i.e. the 'best' $k$ will always be the number of data points, where \emph{kMeansError} is 0, so \emph{kMeansError} is not a good indicator of what $k$ to choose.
 \item For the same reason above. If we choose $k$ based on the \emph{kMeansError} we get from the test data, we would be setting $k$ to the number of data points of the test set, i.e. we'd be overfitting.
 \item Minimum error is 1156, occuring at $k = 10$. (See Figure \ref{fig:q2_2_3})
 \item From the graph, we can see the biggest change in slope happens between $k=3$ and $k=4$, so either of these values for $k$ can be reasonable. 
 }

 \begin{figure}[h!]
    \includegraphics[width=25em]{a2_q2_2_3.png}
    \caption{Min error vs. $k$ Graph (Q2.2.3)}
    \label{fig:q2_2_3}
\end{figure}


 \subsection{$k$-Medians}
 
 \begin{enumerate}
 \item The result puts one of the outliers in a cluster of its own, with the non-outliers being put into 3 clusters. This doesn't make a lot of sense. (See Figure \ref{fig:q2_3_1})
 \item From the graph it looks like the biggest change in slope occurs at $k = 7$ and $k = 8$, so we can reasonably choose $k = 7$. (See Figure \ref{fig:q2_3_2})
 \item See Figure \ref{fig:q2_3_3}. Code added:
 \begin{verbatim}
    # Return L1-norm of  all pairs of rows in X1 and X2
    function LOneNorms(X1,X2)
        (n,d) = size(X1)
        (t,d2) = size(X2)
        assert(d==d2)
        D = zeros(n,t)
        for i in 1:n
            for j in 1:t
                D[i,j] = sum(abs.(X1[i,:] .- X2[j,:]));
            end
        end
        return D
    end

    function kMediansError(X, y, W)
        error = 0;
        (n,d) = size(X);

        for i in 1:n
            error += sum(abs.(X[i,:] .- W[Int(y[i]),:]));
        end

        return error;
    end

    function kMedians(X,k;doPlot=false)
    # K-medians clustering
    
    (n,d) = size(X)
    
    # Choos random points to initialize medians
    W = zeros(k,d)
    perm = randperm(n)
    for c = 1:k
        W[c,:] = X[perm[c],:]
    end
    
    # Initialize cluster assignment vector
    y = zeros(n)
    changes = n
    error = Inf;
    
    while changes != 0
    
        # Compute L1-norms between each point and each mean
        D = LOneNorms(X,W)
    
        # Assign each data point to closest median (track number of changes labels)
        changes = 0
        for i in 1:n
            (~,y_new) = findmin(D[i,:])
            changes += (y_new != y[i])
            y[i] = y_new
        end
    
        # Optionally visualize the algorithm steps
        if doPlot && d == 2
            clustering2Dplot(X,y,W)
            sleep(.1)
        end
    
    
        # Find median of each cluster
        for c in 1:k
            W[c,:] = median(X[y.==c,:],1)
        end
    
        # Optionally visualize the algorithm steps
        if doPlot && d == 2
            clustering2Dplot(X,y,W)
            sleep(.1)
        end
    
        error = kMediansError(X,y,W);
        @printf("Running k-medians, error = %d\n",error)
    end
    
    function predict(Xhat)
        (t,d) = size(Xhat)
    
        D = LOneNorms(Xhat,W)
    
        yhat = zeros(Int64,t)
        for i in 1:t
            (~,yhat[i]) = findmin(D[i,:])
        end
        return yhat
    end
    
    return PartitionModel(predict,y,W,error)
    end
\end{verbatim}
\item From the graph (Figure \ref{fig:q2_3_4}) it looks like the biggest change in slope occurs at $k=3$ to $k=4$, so we can probably set $k = 4$. From theh previous question we know we can get a reasonable clustering with $k=4$, so yes we are satisfied with the result.
\end{enumerate}

\begin{figure}
    \includegraphics[width=30em]{a2_q2_3_1.png}
    \caption{Clustering with minimum error at $k=4$ (Q2.3.1)}
    \label{fig:q2_3_1}

    \includegraphics[width=30em]{a2_q2_3_2.png}
    \caption{Min error vs. $k$ Graph (Q2.3.2)}
    \label{fig:q2_3_2}
\end{figure}
\begin{figure}
    \includegraphics[width=30em]{a2_q2_3_3.png}
    \caption{Clustering of minimum error at $k=4$ (Q2.3.3)}
    \label{fig:q2_3_3}

    \includegraphics[width=30em]{a2_q2_3_4.png}
    \caption{Min error vs. $k$ Graph (Q2.3.4)}
    \label{fig:q2_3_4}
 \end{figure}

\subsection{Very-Short Answer Questions}

\enum{
\item No. As we can see from the results of \texttt{clusterData2.jld} dataset, $k$-means clustering doesn't work very well when there are outliers, and in this specific case, $k$-medians actually works better. In addition, the results of $k$-means largely depend on where we place our intial means, so the performance fo $k$-means can be drastically decreased/increased by choosing bad/good initial means.
\item If we determine $k$ by minimizing the objective function given above, we would always end up with the maximum $k$, i.e. the number of data points in the dataset. This is not useful at all because we are saying each object is its own cluster, which tells us nothing about the data.
\item Something like the \texttt{clusterData2} dataset would be a good example, where there are a few outliers to move one of the means to one of the outliers.
}

\section{More Unsupervised Learning}

\subsection{Density-Based Clustering}
\enum{
\item \texttt{radius = 2, minPoints = 2}
\item \texttt{radius = 4, minPoints = 2}
\item \texttt{radius = 15, minPoints = 5}
\item \texttt{radius = 50, minPoints = 2}
}

\subsection{Vector Quantization}

\begin{enumerate}
\item
\begin{verbatim}
function quantizeImage(fn,b)
    Xo = imread(fn);
    (nRows,nCols) = size(Xo);
    X = reshape(Xo, (nRows*nCols,3));

    model = kMeans(X,2^b,doPlot=false)
    return model.y,model.W,nRows,nCols
end

function deQuantizeImage(y,W,nRows,nCols)
    X = zeros(nRows*nCols,3)
    for i in 1:nRows*nCols
        X[i,:] = W[Int(y[i]),:];
    end

    Xo = reshape(X, (nRows,nCols,3));
    imshow(Xo);
    return Xo
end
\end{verbatim}
\item See Figures \ref{fig:q3_2_1}, \ref{fig:q3_2_2}, \ref{fig:q3_2_3}, \ref{fig:q3_2_4}
\end{enumerate}
\begin{figure}
    \includegraphics[width=30em]{a2_q3_2_1.png}
    \caption{dog.png encoded with 1 bit per pixel(Q.3.3)}
    \label{fig:q3_2_1}

    \includegraphics[width=30em]{a2_q3_2_2.png}
    \caption{dog.png encoded with 2 bits per pixel(Q3.3)}
    \label{fig:q3_2_2}
\end{figure}
\begin{figure}
    \includegraphics[width=30em]{a2_q3_2_3.png}
    \caption{dog.png encoded with 4 bits per pixel(Q3.3)}
    \label{fig:q3_2_3}

    \includegraphics[width=30em]{a2_q3_2_4.png}
    \caption{dog.png encoded with 6 bits per pixel(Q3.3)}
    \label{fig:q3_2_4}
 \end{figure}
\subsection{Very-Short Answer Questions}

\enum{
\item The feature with the smaller scale would have very little influence over whether a point is 'reachable' or not. For example, a point can be very close (~1:1 ratio) to another point on the large-scale feature but very far (several times away) on the small-scale feature, but it might still be 'reachable' from the other point because the smaller-scale feature difference won't add a lot to the overall distance.
\item Advantage: we can actually tell the algorithm what is considered an 'outlier' instead of having the algorithm figure out what it is, where it may potentially be determined to be something else. Drawback: it takes a lot of time to manually label the outliers in the training dataset.
\item Essentially, we are looking for points that are within the distance $r$ from the test query point on a 2D grid. This will take $O(n)$ time because we have to go through each point. If we use the hash table approach, assuming the hash table is already filled out, this approach will require us to find the square that contains the test query and its 8 neighbors, which would take constant time since they're all stored in a hash table. Then we would have to go through the elements in each of the 9 grids to check if they are within $r$ units from the test query, so this approach will take maximum $9k$ time, i.e. $O(k)$.
}

\section{Matrix Notation and Linear Regression}

\subsection{Converting to Matrix/Vector/Norm Notation}

\begin{enumerate}
     \item  $ \norm{Xw-y}$
    \item  $ \norm{Xw-y}_\infty +   \frac{\lambda}{2}\norm{w}_2$
    \item  $ (Xw-y)^T V(Xw-y) +   \lambda\norm{w}_1$
\end{enumerate}


\subsection{Minimizing Quadratic Functions as Linear Systems}

\begin{enumerate}
     \item  $f(w) = \frac{1}{2}\norm{w-u}^2$\\
                      	$ = \frac{1}{2}(w-u)^T (w-u)$\\
                     	$ = \frac{1}{2}(w^T-u^T) (w-u)$\\
				$ = \frac{1}{2}(w^T w - 2w^T u + u^T u)$\\
                      	$= \frac{1}{2}\norm{w}^2 - w^T u +  \frac{1}{2}\norm{w}^2$\\
 				 $\nabla f(w) = w-u$ \\
               
    \item   $f(w) = \frac{1}{2}\norm{w}^2 + w^TX^Ty$\\
                   $ = \frac{1}{2}(w^T w) + w^TX^Ty$\\
			  $ \nabla f(w) =  w+ X^Ty$\\
   
      \item   $f(w)= \frac{1}{2}\norm{Xw - y}^2 + \frac{1}{2}w^T\Lambda w$\\
                 $= \frac{1}{2}(w^TX^T-y^T)(Xw-y) + \frac{1}{2}w^T\Lambda w$\\
			    $= \frac{1}{2}((Xw)^T(Xw)-2y(Xw)-y^Ty) + \frac{1}{2}w^T\Lambda w$\\
 			$= \frac{1}{2}w^T(x^Tx)w - w^Tx^Ty-  \frac{1}{2}y+ \frac{1}{2}w^T\Lambda w$\\
                 $ \nabla f(w) = X^TXw- X^Ty+\Lambda w$\\


        \item  $f(w) = \frac{1}{2}\sum_{i=1}^n v_i (w^Tx_i - y_i)^2$\\
                 $ = \frac{1}{2}( (Xw-y)^TV(Xw-y)$\\
                    $ = \frac{1}{2}( (w^TX^T-y^T)^V(Xw-y)$\\
                    $ = \frac{1}{2}(w^TX^TVXw - y^TVXw- w^TX^TVy+y^TVy)$\\
                     $ = \frac{1}{2}(w^TX^TVXw) -  (w^TX^TVy)+ \frac{1}{2}(y^TVy)$\\
  			$ \nabla f(w) = X^TVXw- X^TVy$\\

 
\end{enumerate}



%In class we discuss fitting a linear regression model by minimizing the squared error. 
%This classic model is the simplest version of many of the more complicated models we will discuss in the course. However, it typically performs very poorly in practice. One of the reasons it performs poorly is that it assumes that the target $y_i$ is a linear function of the features $x_i$ with an intercept of zero. This drawback can be addressed by adding a bias variable and using nonlinear bases (although nonlinear bases may increase to over-fitting). 

%In this question, you will start with a data set where least squares performs poorly. You will then explore how adding a bias variable and using nonlinear (polynomial) transforms can drastically improve the performance. You will also explore how the complexity of a basis affects both the training error and the test error. In the final part of the question, it will be up to you to design a basis with better performance than polynomial bases. If you are not familiar with Matlab, to get you started please see the notes on Matlab commands on the course webpage.

\subsection{Linear Regresion with Bias Variable}

\begin{enumerate}


\item See Figure \ref{fig:q4_3}
\begin{verbatim}

function leastSquaresBias(X,y)

     (n,)= size(X)
     X0 =  addBasis(X)
    # Find regression weights minimizing squared error
    w = (X0'*X0)\(X0'*y)

    # Make linear prediction function
    predict(Xhat) =   addBasis(Xhat)*w

    # Return model
    return GenericModel(predict)
end

function addBasis(X)
    (n,)= size(X)
    X0 = ones(n,2)
    for i in 1:n
        X0[i,2]= X[i]
     end 
   return X0
end 

 \end{verbatim}

\item
 \begin{verbatim}
Squared train Error with least squares: 3551.346
Squared test Error with least squares: 3393.869
 \end{verbatim}


\end{enumerate}


\begin{figure}[h!]
    \includegraphics[width=30em,height=7.5cm]{a2_q4_3.png}
    \caption{Least Squares Bias (Q4.3)}
    \label{fig:q4_3}
  \end{figure}

\vspace{10em}
\subsection{Linear Regression with Polynomial Basis}

\begin{enumerate}
 
\item
\begin{verbatim}
function leastSquaresBasis(X,y,p)

     (n,)= size(X)
      X0 = polyBasis(X,p)


    # Find regression weights minimizing squared error
    w = (X0'*X0)\(X0'*y)

    # Make linear prediction function
    predict(Xhat) = polyBasis(Xhat,p)*w

    # Return model
    return GenericModel(predict)
end

function polyBasis(X,p)
    (n,)= size(X)
    X0 = ones(n,p+1)
    for i in 1:n
        X0[i,2]= X[i]
     end 
   for i in 3:p+1
      X0[:,i]= X0[:,2].^(i-1)
   end
   return X0
end 
 \end{verbatim}


\item
 \begin{verbatim}
Squared train Error with p: 1 and least squares: 3551.346
Squared test Error with p: 1 and least squares: 3393.869
Squared train Error with p: 2 and least squares: 2167.992
Squared test Error with p: 2 and least squares: 2480.725
Squared train Error with p: 3 and least squares: 252.046
Squared test Error with p: 3 and least squares: 242.805
Squared train Error with p: 4 and least squares: 251.462
Squared test Error with p: 4 and least squares: 242.126
Squared train Error with p: 5 and least squares: 251.143
Squared test Error with p: 5 and least squares: 239.545
Squared train Error with p: 6 and least squares: 248.583
Squared test Error with p: 6 and least squares: 246.005
Squared train Error with p: 7 and least squares: 247.011
Squared test Error with p: 7 and least squares: 242.888
Squared train Error with p: 8 and least squares: 241.306
Squared test Error with p: 8 and least squares: 245.966
Squared train Error with p: 9 and least squares: 235.762
Squared test Error with p: 9 and least squares: 259.296
Squared train Error with p: 10 and least squares: 235.074
Squared test Error with p: 10 and least squares: 256.300
 \end{verbatim}

\item As p increasing, the training error and testing error both decrease until $p$ reaches 8, at which point we start to overfit so the testing error starts to increase even as the training error goes down.

\end{enumerate}



\subsection{Manual Search for Optimal Basis}

\begin{enumerate}
\item
 \begin{verbatim}

function leastTanBasis(X,y)

     (n,)= size(X)
      X0 = tanBasis(X)


    # Find regression weights minimizing squared error
    w = (X0'*X0)\(X0'*y)

    # Make linear prediction function
    predict(Xhat) = tanBasis(Xhat)*w

    # Return model
    return GenericModel(predict)
end

function tanBasis(X)
    (n,)= size(X)
    X0 = ones(n,12)

    for i in 1:n
        X0[i,2] =X[i]
        X0[i,3]= tan(0.1357*X[i])
        X0[i,4]= tan(0.1357*X[i]).^2
        X0[i,5]= tan(0.1357*X[i]).^3
        X0[i,6]= tan(0.1357*X[i]).^4
        X0[i,7]= tan(0.1357*X[i]).^5
        X0[i,8]= tan(0.1357*X[i]).^6
        X0[i,9]= tan(0.1357*X[i]).^7
        X0[i,10]= tan(0.1357*X[i]).^8
        X0[i,11] = sin(X[i])  
        X0[i,12] = sin(X[i]).^2  
         
     end 

   return X0
end

 \end{verbatim}


\item

  \begin{verbatim}
Squared train Error with least squares: 241.804 
Squared test Error with least squares: 368.812
  \end{verbatim}

\end{enumerate}

\subsection{Very-Short Answer Questions}

\begin{enumerate}
\item Linear regression is usually for continuous data. It is impossible for line to pass through all the data points so testing equality would result in a practically useless and very high error. And aslo, linear regression is not predicting the exact number.
\item 
When two features that are collinear, the least squares estimate would not be unique. This is because as long as the sum of the weights of these features remains the same, we wouldn't be changing the predictions, but that also means we can change the weights in (infinitely) many was without changing the predictions, so there wouldn't be a unique minimum.

\item 
The computation needed for closed-form is \texttt{W = (X0'*X0)/(X0'*y)}. 
The runtime of  \texttt{(X0'*X0)} is $O(np^2)$, the runtime of \texttt{(X0'*y)} is $O(np)$ and  
the runtime of solving \texttt{W} is $O(p^3)$, so the total runtime  = $O(p^3) +O(np^2) + O(np)$.
Since $O(np^2)$ domidates $O(np)$ , and in generl $n>p$, therefore, total runtime = $O(np^2)$.


\item
Regression tree is suitable for data with many clusters (hard to fit one line or curve). Regression trees could split the data into different clusters and run linear regression with least squares on each cluster in the leaves as opposed to running least squares on the whole dataset.

\end{enumerate}

\section{Robust Regression and Gradient Descent}

\subsection{Weighted Least Squares in One Dimension}

See Figure \ref{fig:q5_1}\\

  \begin{verbatim}

function weigthedleastSquares(X,y,v)
    (n,) = size(v)
    V =  zeros(n,n)
     
    for i in 1:n
        for j in 1:n
            if i==j
                V[i,j]=v[i]
              end 
        end
    end
    
    w = (X'*V*X)\(X'*V*y)
    # Make linear prediction function
    predict(Xhat) = Xhat*w

    # Return model
    return GenericModel(predict)
end

  \end{verbatim}

\begin{figure}[h!]
  \includegraphics[width=30em,height=7.5cm]{a2_q5_1.png}
  \caption{Weighted Least Squares (Q5.1)}
  \label{fig:q5_1}
\end{figure}



\subsection{Smooth Approximation to the L1-Norm}

$\nabla _w(log(exp(w^Tx_i-y_i)+exp(y_i-w^Tx_i))= \frac{x_i(exp(w^Tx_i-y_i)-exp(y_i-w^Tx_i))}{(exp(w^Tx_i-y_i)+exp(y_i-w^Tx_i))ln(e)}$\\
$\nabla f= \sum\limits_{i=1}^{n}\frac{x_i(exp(w^Tx_i-y_i)-exp(y_i-w^Tx_i))}{(exp(w^Tx_i-y_i)+exp(y_i-w^Tx_i))}$

\subsection{Robust Regression}

\begin{enumerate}

\item
  \begin{verbatim}
function leastSquaresObj(w,X,y)
    f = sum(log(exp.(X*w-y)+exp.(y-X*w)))
    g = X'*((exp.(X*w-y)-exp.(y-X*w))./(exp.(X*w-y)+exp.(y-X*w)))
    return (f,g)
end

  \end{verbatim}

\end{enumerate}

\begin{figure}[h!]
    \includegraphics[width=30em,height=7.5cm]{a2_q5_3.png}
    \caption{Smooth approximation (Q5.3)}
    \label{fig:q5_3}
  \end{figure}

\subsection{Very-Short Answer Questions}

\enum{
\item 1. Graph-based: this would be effective for this dataset because the dataset is in 2D, which means it's easy to make into a human readable graph. And from the graph we can easily tell where the outliers are in a very short amount of time. Even though labelling might take a while, this data set is relatively small so it's definitely doable. 2. Model-based: it won't work as well because the mean and variance are sensitive to outliers, which we have a lot here. If we use this approach, the mean would likely end up somewhere close to the middle, which means it will only detect some of the outliers at the top but likely not the ones closer to the middle.
\item When $d$ (number of features) gets too large. This is because the closed form solution takes $O(nd^2+d^3)$ time, so it scales up very quickly as $d$ goes up, due to the squares and cubes, whereas gradient descent takes $O(ndt)$ time so it doesn't scale up as much.
\item The absolute value graph may not have a point where gradient is 0, as it keeps going down and then back up again in a sharp corner. Due to the discontinuity the gradient at the minimum may actually be undefined so cannot be solved by the gradient method. If we smooth the absolute value though, the sharp corner goes away and the function becomes continuous, which means we can actually find the local min with gradient descent.
}
\end{document}
