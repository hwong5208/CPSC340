\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{listings} % For displaying code
\usepackage{algorithm2e} % pseudo-code

% Answers
\def\ans#1{\par\gre{Answer: #1}}
%\def\ans#1{} % Comment this line to produce document with answers

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{a1f/#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{a1f/#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}
\newcommand{\matCode}[1]{\lstinputlisting[language=Matlab]{a1f/#1.m}}


\begin{document}

\title{CPSC 340 Assignment 1 (due Friday September 29 ATE)}
\author{}
\date{}
\maketitle
\vspace{-4em}


Name: Hang Yee Wong\\
CS ID: r9i0b 

Name: Joanne Chen\\
CS ID: r0a9

The assignment instructions are the same as Assignment 0, except you have the option to work in a group of 2. If you work in a group, please only hand in \emph{one} assignment. It is recommended that you work in groups as the assignment is quite long, but please only submit one assignment for the group and make sure that everyone's name/ID is on the front page.


\section{Summary Statistics and Data Visualization}

Download and expand the file \emph{a1.zip}, which contains estimates of the influenza-like illness percentage over 52 weeks on 2005-06 by Google Flu Trends in a comma-separated values (CSV) file. You can open this with Excel or other spreadsheet programs; the first row gives the abbreviation of the region names for each column, and each row gives the estimate for a week.
After you change to the a0 directory, you can load this data in Julia using:
\begin{verbatim}
dataTable = readcsv("fluTrends.csv")
\end{verbatim}
This creates an two-dimensional array of type ``Any''  populated with all the information in the CSV file.

\subsection{Summary Statistics}

\blu{Report the following statistics}: the minimum, maximum, mean, median, and mode of all values across the dataset. In light of thea above, \blu{is the mode a reliable estimate of the most ``common" value? Describe another way we could give a meaningful ``mode" measurement for this (continuous) data.}

Hint: Since the first row of the CSV file is just the names of the columns, we can create a matrix $X$ containing the data stored as real numbers using:
\begin{verbatim}
X = real(dataTable[2:end,:])
\end{verbatim}
Also, Julia has no mode command so I've included a mode function in `misc.jl'.

\begin{verbatim}

minimum(X) =  0.352

maximum(X) = 4.862

mean(X) = 1.3246249999999997

median(X) = 1.1589999999999998

mode(X) = 0.77

\end{verbatim}

The mode isn't very reliable because a lot of these data points have the same integer values but different decimal values. To give a meaningful mode, we can make the data discrete by rounding the data points.



\subsection{Data Visualization}

Consider the figure on the next page.
The figure contains the following plots, in a shuffled order:
\enum{
\item A histogram showing the distribution of each the values in the matrix $X$.
\item A boxplot grouping data by weeks, showing the distribution across regions for each week.
\item A scatterplot between the two regions with highest correlation.
\item A single histogram showing the distribution of \emph{each} column in $X$.
\item A scatterplot between the two regions with lowest correlation.
\item A plot containing the weeks on the $x$-axis and the percentages for each region on the $y$-axis.
}
\blu{Match the plots (labeled A-F) with the descriptions above (labeled 1-6), with an extremely brief (a few words is fine) explanation for each decision.}

Hint: you can generate similar plots by adding the PyPlot package. To add this package use:
\begin{verbatim}
Pkg.add("PyPlot") # Do this once per computer
using PyPlot # Do this once per session
plot(1:52,X[:,1]) # Plot the first row
\end{verbatim}
To generate similar-looking plots you can use the functions `plot', `boxplot', `plt[:hist]', and `scatter'.

\begin{enumerate}
\item C
\item B
\item F
\item D
\item E
\item A
\end{enumerate}


\subsection{Decision Surfaces}

Consider the figure below, which plots a set of two-dimensional training examples and the decision surface produced by a ``neural network'' classifier (a model we'll see later in the course).
\blu{How many training examples has the neural network mis-classified?} (This figure is best viewed in colour.)

There are 9 green crosses on the side classified to be blue and 8 blue circles on the green side, so a total of 17 examples were mis-classified.

\section{Decision Trees}


\subsection{Equality vs. Inequality Splitting Rules}

It might make more sense when working with features with few, discrete values that are not related. For example, with 'colour' as a feature converted to numbers, inequalities wouldn't make a lot of sense as the numbers representing the colours are not related in quantity.

\subsection{Decision Stump Implementation}

Training Error of \texttt{decisionStump}: 0.25

\subsection{Constructing Decision Trees}
\begin{verbatim}
if latitude > 37.669007
    if longitude > -96.090109
        return 1
    else
        return 2
else
    if longitude > -115.577574
        return 2
    else
        return 1
\end{verbatim}



\subsection{Cost of Fitting Decision Trees}

The first (top-level) decision stump takes $O(nd\log{n})$ time, and as we split, the second-level decision stumps each takes $O(nd\log{\frac{n}{a}})$ and $O(nd\log{\frac{n}{b}})$ time, where $\frac{n}{a} + \frac{n}{b} = n$. So the second layer takes $O(nd\log{n})$ time still. We apply the same reasoning to each layer, and we have a total of $m$ layers, so the total cost is $O(mnd\log{n})$ time.

\section{Training and Testing}

\subsection{Training Error}

Initially, the training error of \texttt{decisionStump} decreases as the depth increases, but it stops decreasing after depth 4. On the other hand, the training error of infogain decreases with increasing depth all the way to zero.

The error stops decreasing after a certain depth for the decision tree model likely because the splitting isn't even and sometimes the NO case has no more objects even when there are still objects of different labels in the same YES case. When this happens, we have reached the base split case and our implementation of \texttt{decisionTree} will split no further, meaning that some of the objects that end up in the same YES-leaf have different labels, which means some of them will be assigned labels different from the actual labels they have, which increases the training error regardless of what value is assigned to them.

\subsection{Training and Testing Error Curves}

\textit{
Legend:
\begin{itemize}
	\item blue crosses are training errors
	\item red dots are test errors
	\item x-axis represents the depth
	\item y-axis represents the error
\end{itemize}}

\begin{figure}[h!]
  \includegraphics[height=7.5cm]{Etrain_Etest_vs_depth.png}
  \caption{Training Error and Test Error vs. Depth}
  \label{fig: error vs depth}
\end{figure}


\subsection{Validation Set}

If we split the dataset in half and use the first half for training and second half for validation, the minimum validation set error we were able to obtain was 0.165 at depth 4 with a 0.110 train error, but after depth 4 the validation set error stays the same. If we switch the data sets and use the second half for training and first half for validation, the minimum validation error becomes 0.155 at depth 4 with a 0.105 training error. To use more of our data to estimate depth, we could use cross validation so every object is used for validation once.

\section{Naive Bayes}

In this section we'll implement naive Bayes, a very fast classification method that is often surprisingly accurate for text data with simple representations like bag of words.



\subsection{Naive Bayes by Hand}

Consider the dataset below, which has $10$ training examples and $2$ features:
\[
X = \begin{bmatrix}0 & 1\\1 & 1\\ 0 & 0\\ 1 & 1\\ 1 & 1\\ 0 & 0\\  1 & 0\\  1 & 0\\  1 & 1\\  1 &0\end{bmatrix}, \quad y = \begin{bmatrix}1\\1\\1\\1\\1\\1\\0\\0\\0\\0\end{bmatrix}.
\]
Suppose you believe that a naive Bayes model would be appropriate for this dataset, and you want to classify the following test example:
\[
\hat{x} = \begin{bmatrix}1 & 0\end{bmatrix}.
\]

\blu{(a) Compute the estimates of the class prior probabilities} (you don't need to show any work):
\items{
\item$ p(y = 1)$.
\item $p(y = 0)$.
}


\begin{enumerate}
\item  $\frac{6}{10}$
\item  $\frac{4}{10}$

\end{enumerate}

\blu{(b) Compute the estimates of the 4 conditional probabilities required by naive Bayes for this example}  (you don't need to show any work):
\items{
\item $p(x_1 = 1 | y = 1)$.
\item $p(x_2 = 0 | y = 1)$.
\item $p(x_1 = 1 | y = 0)$.
\item $p(x_2 = 0 | y = 0)$.
}

\begin{enumerate}
\item  $\frac{1}{2}$
\item  $\frac{1}{3}$
\item  $1$
\item  $\frac{3}{4}$

\end{enumerate}

\blu{(c) Under the naive Bayes model and your estimates of the above probabilities, what is the most likely label for the test example? (Show your work.)}



$p( y = 1 |  x_1 = 1 , x_2 = 0 ) \propto p(  x_1 = 1  | y = 1 ) p(  x_2 = 0  | y = 1 ) p( y =1)$.
        \newline =$ (\frac{1}{2}) (\frac{1}{3})(\frac{6}{10}) $
               \newline = $0.1$
\newline  
$p( y = 0 |  x_1 = 1 , x_2 = 0 ) \propto p(  x_1 = 1  | y = 0 ) p(  x_2 = 0  | y = 0 ) p( y =0)$.
        \newline =$ (1) (\frac{3}{4})(\frac{4}{10}) $
               \newline = $0.3$
\newline 
Since the $p( y = 0 |  x_1 = 1 , x_2 = 0 ) $ is bigger , the most likely label is 0.


\subsection{Bag of Words}

If you run the script \emph{example\_bagOfWods.jl}, it will load the following dataset:
\enum{
\item $X$: A sparse binary matrix. Each row corresponds to a newsgroup post, and each column corresponds to whether a particular word was used in the post. A value of $1$ means that the word occured in the post.
\item $wordlist$: The set of words that correspond to each column.
\item $y$: A vector with values $1$ through $4$, with the value corresponding to the newsgroup that the post came from.
\item $groupnames$: The names of the four newsgroups.
\item $Xvalidate$ and $yvalidate$: the word lists and newsgroup labels for additional newsgroup posts.
}
\blu{Answer the following}:
\enum{
\item Which word is present in the newsgroup post if there is a $1$ in column 50 of X?
\item Which words are present in training example 500?
\item Which newsgroup name does training example 500 come from?
}

\begin{enumerate}
\red{
\item  $"league"$
\item  $"car", "engine", "evidence", "problem", "system"$
\item $"rec.*"$
}

\end{enumerate}


\subsection{Naive Bayes Implementation}

If you run the function \emph{example\_decisionTree\_newsgroups.jl} it will load the newsgroups dataset and report the test error for decision trees of different sizes (it may take a while for the deeper trees, as this is a sub-optimal implementation). On other other hand, if you run the function \emph{example\_naiveBayes.jl} it will fit the basic naive Bayes model and report the test error.

While the \emph{predict} function of the naive Bayes classifier is already implemented, the calculation of the variable $p\_xy$ is incorrect (right now, it just sets all values to $1/2$). \blu{Modify this function so that \emph{p\_xy} correctly computes the conditional probabilities of these values based on the frequencies in the data set. Hand in your code and report the test error that you obtain.}




\subsection{Runtime of Naive Bayes for Discrete Data}

Assume you have the following setup:
\items{
\item The training set has $n$ objects each with $d$ features.
\item The test set has $t$ objects with $d$ features.
\item Each feature can have up to $c$ discrete values (you can assume $c \leq n$).
\item There are $k$ class labels (you can assume $k \leq n$)
}
You can implement the training phase of a naive Bayes classifier in this setup in $O(nd)$, since you only need to do a constant amount of work for each $X(i,j)$ value. (You do not have to actually implement it in this way for the previous question, but you should think about how this could be done). \blu{What is the cost of classifying $t$ test examples with the model?}

The cost of classify is O(tdk). Because the dominant cost is computing p(Xij | Yi). And we can have it with three for loops. Loop over the example t, and loop over the features d, and loop over the class labels k .


\section{K-Nearest Neighbours}

\subsection{KNN Prediction}

\enum{
	\item See code in \texttt{knn.jl}
	\item $x(k, Etrain, Etest) = (1, 0.000, 0.286), (3, 0.028, 0.286), (10, 0.072, 0.286)$
	\item See Figure 2 below
	\item This is because the there is only one nearest neighbour and the nearest neighbour of any point in \texttt{Xhat} is always itself as \texttt{Xhat} is equal to \texttt{X} when calculating the training error. This means the predicted label is always the same as the actual label, meaning the training error = 0.
	\item Use cross validation to choose the best $k$. Using the \texttt{citiesSmall} dataset a fold of 5 would probably make sense.
} 
\begin{figure}[h!]
  \includegraphics[height=7.5cm]{knn_train_data_k=1.png}
  \caption{KNN Training Data on k=1}
  \label{fig: knn}
\end{figure}

\subsection{Condensed Nearest Neighbours}

\enum{
	\item After training \texttt{cknn} on \texttt{X} of \texttt{citiesBig1}, running \texttt{predict} on \texttt{Xtest} of \texttt{citiesBig1} with $k = 1$ took 4.136411572 seconds. Running the same data using \texttt{knn} with the same $k$ took 140.647040979 seconds.
	\item Size of the subset = 457, $Etrain$ = 0.008, $Etest$ = 0.018
	\item This is because an object in the training data set might not be in the model anymore, so the nearest neighbour of an object might not always be itself, meaning the predicted label might not always match the actual label.
	\item We need to calculate the distance between each object in \texttt{Xhat} and each object in \texttt{Xsub}, so we need to calculate $ts$ distances in total. Each distance is calculated from $d$ dimensions, so the total cost is $O(tsd)$.
    \item $(k, Etrain, Etest) = (1, 0.138, 0.210), (3, 0.285, 0.227), (10, 0.571, 0.512)$. The high training the test errors may be due to the fact that a lot of the objects that are close to each other actually have different labels, so using knn in this case may not be reliable. //TODO
}

\section{Very-Short Answer Questions}

\blu{Write a short one or two sentence answer to each of the questions below}. Make sure your answer is clear and concise.

\enum{
\item What is one reason we would want to look at scatterplots of the data before doing supervised learning?
\item What is a reason that the examples in a training and test set might not be IID?
\item What is the difference between a validation set and a test set?
\item Why is  naive Bayes called ``naive''?
\item What is a situation where the naive Bayes assumption could lead to poor performance?
\item What is the main advantage of non-parametric models?
\item A standard pre-processing step is ``standardization'' of the features: for each column of $X$ we subtract its mean and divide by its variance. Would this pre-processing change the accuracy of a decision tree classifier? Would it change the accuracy of a KNN classifier?
\item Does increasing $k$ in KNN affect the training or prediction asymptotic runtimes?
\item How does increase the parameter $k$ in $k$-nearest neighbours affect the two parts (training error and approximation error) of the fundamental trade-off (hint: think of the extreme values).
\item For any parametric model, how does increasing number of training examples $n$ affect the two parts of the fundamental trade-off.
}



\begin{enumerate}
\red{
\item  Scatterplot could provide a brief idea of that data. For example, it could help us choose a better k in knn and a better depth in decision trees.
\item  If a data set is not chosen randomly, it is not IID. For example, some data points might be sampled much closer in time than other data points.
\item Validation set is a part of our training data used to approximate test error. Test set is for evaluation of the model and to provide the true test error, and we cannot access it during training.
\item Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.
\item The features of data are not independent of each other. For example, in bag of words, words that represent postal codes, addresses, and cities have a higher chance of appearing together than other words.
\item Non-parametric models don't grow with the size of the data set, so they tend to be a lot faster than parametric models when working with huge data sets.
\item //TODO It will affect the accuracy, because it change the data 's distinace. 
\item It will not affect the training and prediction. In the traing phrase, KNN always stores all the data in the model, regardless of what k is. In the prediction phase, KNN always computes all the distances between a point and all the other points, so k really only changes the runtime when we are taking the mode of the k nearest neighbours in the prediction phase, but because k is a constant and is typically much smaller than n and d, it will not affect the overall runtime asymptotically.
\item Training error increases as k increases (training error is 0 when k = 1). The approximation error decreases with k up to a certain threshold, then starts increasing after the threshold as it starts to take outliers into account.
\item For large n, the training error would increase and the approximation error would decrease because the model would become more general as n increases.
}


\end{enumerate}


\end{document}
