\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{listings} % For displaying code
\usepackage{algorithm2e} % pseudo-code

% Answers
\def\ans#1{\par\gre{Answer: #1}}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{a3f/#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{a3f/#2}\end{center}}
\newcommand{\matCode}[1]{\lstinputlisting[language=Matlab]{a3f/#1.m}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}


\begin{document}

\title{CPSC 340 Assignment 3 (due November 3 ATE)}
\author{}
\date{}
\maketitle
\vspace{-4em}



\section{Convex Functions}

Recall that convex loss functions are typically easier to minimize than non-convex functions, so it's important to be able to identify whether a function is convex.

\blu{Show that the following functions are convex}:

\enum{
\item $f(w) = \alpha w^2 - \beta w + \gamma$ with $w \in \R, \alpha \geq 0, \beta \in R, \gamma \in R$ (1D quadratic).
\item $f(w) = w\log(w) $ with $w > 0$ (``neg-entropy'')
\item $f(w) = \norm{Xw-y}^2 + \lambda\norm{w}_1$ with $w \in \R^d, \lambda \geq 0$ (L1-regularized least squares).
\item $f(w) = \sum_{i=1}^n \log(1+\exp(-y_iw^Tx_i)) $ with $w \in \R^d$ (logistic regression).
\item $f(w,w_0) = \sum_{i=1}^N\max\{0,w_0 - w^Tx_i\} - w_0 + \frac{\lambda}{2}\norm{w}_2^2$  with $w \in R^d, w_0 \in \R, \lambda \geq 0$ (``1-class'' SVM).
}

Hint: for the first two you can use the second-derivative test since they are one-dimensional. For the last 3 you'll have to use some of the results in class regarding how combining convex functions  can yield convex functions. For the logistic regression case, a similar case would be showing that $\log(\exp(x))$ is convex even though $\log(x)$ is a concave function.

\enum{
\item $f(w) = \alpha w^2 - \beta w + \gamma$\\
         $f'(w) = 2\alpha w  - \beta $\\
          $f''(w) = \alpha w   >0  $
\item $f(w) = w\log(w) $ with $w > 0$\\ 
       $f'(w) = w(\frac{1}{w})+ \log(w) $\\
       $f'(w) = 1+ \log(w) $ \\
       $f''(w) = \frac{1}{w}  >0 $
\item $f(w) = \norm{Xw-y}^2 + \lambda\norm{w}_1$ \\
        we have $ \norm{Xw-y}^2$ is a convex function because it is norm.\\
       And also, $\lambda\norm{w}_1$  is a convex function because it is norm and $\lambda \geq 0$.\\ Therefore, $\lambda\norm{w}_1$ is convex. The $f(w)$ is two sum of convex funciton so it is convex. 
\item $f(w) = \sum_{i=1}^n \log(1+\exp(-y_iw^Tx_i))$\\
      Let  $g(z) = \log(1+\exp(Z))$\\
        $g'(z) = \frac{exp(z)}{1+exp(z)}$\\
         $g'(z) = \frac{1}{1+exp(-z)}$\\
        $g''(z) =  \frac{exp(-z)}{(1+exp(-z))^2}   > 0 $
\item $f(w,w_0) = \sum_{i=1}^N\max\{0,w_0 - w^Tx_i\} - w_0 + \frac{\lambda}{2}\norm{w}_2^2$\\
        $\max\{0,w_0 - w^Tx_i\} $ is a max of convex function, which  is a convex function.\\ And , $\sum_{i=1}^N\max\{0,w_0 - w^Tx_i\} - w_0  $ is a composition of a convex function and linear function , which is a convex function. $\norm{w}_2^2$ is norm , and $\lambda \geq 0$ Therefore, $ \frac{\lambda}{2}\norm{w}_2^2$\ is convex function.
Therefore, $f(w)$ is a convex functon.
}


\section{Gaussian RBFs and Regularization}

Unfortunately, in practice we often don't know what basis to use. However, if we have enough data then we can make up for this by using a basis that is flexible enough to model any reasonable function. These may perform poorly if we don't have much data, but can perform almost as well as the optimal basis as the size of the dataset grows. In this question you will explore using Gaussian radial basis functions (RBFs), which have this property. These RBFs depend on a parameter $\sigma$, which (like $p$ in the polynomial basis) can be chosen using a validation set. In this question, you will also see how cross-validation allows you to tune parameters of the model on a larger dataset than a strict training/validation split would allow.

\subsection{Regularization}

If you run the demo \emph{example\_RBF.jl}, it will load a dataset and split the training examples into a ``train" and a ``validation" set (it does this randomly since the data is sorted). It will then search for the best value of $\sigma$ for the RBF basis. Once it has the ``best" value of $\sigma$, it re-trains on the entire dataset and reports the training error on the full training set as well as the error on the test set.

A strange behaviour appears: if you run the script more than once it might choose different values of $\sigma$. Sometimes it chooses a large value of $\sigma$ that follows the general trend but misses the oscillations. Other times it sets $\sigma = 1$ or $\sigma=2$, which fits the oscillations better but overfits and gives a much higher test error. \blu{Modify the \emph{leastSquaresRBF} function so that it allows a regularization parameter $\lambda$ and it fits the model with L2-regularization. Hand in your code, and report and describe how the performance changes if you use a regularized estimate with $\lambda = 10^{-12}$ (a very small value).}

\begin{verbatim}
function leastSquaresRBF(X,y,sigma)
	lamba = 1/10^(12)
	(n,d) = size(X)

	Z = rbf(X,X,sigma)


 	w = (Z'*Z+lamba)\(Z'*y)
 
	predict(Xhat) = rbf(Xhat,X,sigma)*w

	return LinearModel(predict,w)
end

function rbf(Xhat,X,sigma)
	(t,d) = size(Xhat)
	n = size(X,1)
	D = distancesSquared(Xhat,X)
	return (1/sqrt(2pi*sigma^2))exp.(-D/(2sigma^2))
end
\end{verbatim}

\enum{
\item After used regularized estimated, sometime it will still choose a larges value of  $\sigma$  that follows the general trend but misses the oscillations. However, when it chose $\sigma = 1$ or $\sigma=2$, it will fits the oscillations better and gives a much lower test error.
}

\subsection{Cross-Validation}

While the method rarely performs too badly with regularization, but it's clear that the randomization of the training/validation sets has an effect on the value of $\sigma$ that we choose. This variability would be reduced if we had a larger ``train" and ``validation" set, and one way to simulate this is with \emph{cross-validation}. \blu{Modify the training/validation procedure to use 10-fold cross-validation to select $\sigma$, and hand in your code. How does this change the performance when fixing $\lambda = 10^{-12}$?}\footnote{In practice, we typically use cross-validation to choose both $\sigma$ and $\lambda$}

\begin{verbatim}
minErr = Inf
nsplits = 10
bestSigma = []
for sigma in 2.0.^(-15:15)
   validError =0
   for splits in 1:nsplits
    validStart = Int64(1+(n/nsplits)*(splits-1)) # Start of validation indices
	validEnd = Int64((n/nsplits)*splits) # End of validation incides
	validNdx = perm[validStart:validEnd] # Indices of validation examples
	trainNdx = perm[setdiff(1:n,validStart:validEnd)] 
	Xtrain = X[trainNdx,:]
	ytrain = y[trainNdx]
	Xvalid = X[validNdx,:]
	yvalid = y[validNdx]


	# Train on the training set
	model = leastSquaresRBF(Xtrain,ytrain,sigma)

	# Compute the error on the validation set
	yhat = model.predict(Xvalid)
	#validError = sum((yhat - yvalid).^2)/(n/2)
    validError = sum((yhat - yvalid).^2)/(n/nsplits)
 end 
\end{verbatim}

\enum{
  \item Using cross-validation, almost of the time we will get the best $\sigma$ to 1.
}


\subsection{Cost of Non-Parametric Bases}

When dealing with larger datasets, an important issue is the dependence of the computational cost on the number of training examples $n$ and the number of features $d$. 
\blu{
\enum{
\item What is the cost in big-O notation of training a linear regression model with Gaussian RBFs on $n$ training examples with $d$ features (for fixed $\sigma$ and $\lambda$)? 
\item What is the cost of classifying $t$ new examples with this model? 
\item When is it cheaper to train using Gaussian RBFs than using the basic linear basis? 
\item When is it cheaper to test using Gaussian RBFs than using the basic linear basis?
}}

\enum{
\item  The training cost of Gaussian RBF is O( $n^2d+n^3$). \\
         The training cost of  basic linear basis is O( $nd^2+d^3$).
\item $O(tnd)$
\item  when $ n < d$
 \item  RBF are never cheaper to test.
}



\section{Logistic Regression with Sparse Regularization}

\subsection{Logistic Regression}

Using least squares the train error, validation error, and number of features are: 0.038, 0.106, 101 respectively. Using logistic regression the train error, validation error, and number of features are: 0.0, 0.082, 101 respectively. From the results, we can see that both the train and validation errors go down when we switch to logistic regression, but the number of features selected remains the same.

\subsection{L2-Regularization}

\begin{verbatim}
function logisticObjL2(w,X,y,lambda)
    yXw = y.*(X*w)
    f = sum(log.(1 + exp.(-yXw))) + (lambda/2)*sum(w.*w)
    g = -X'*(y./(1+exp.(yXw))) + lambda*w
    return (f,g)
end

function logRegL2(X,y,lambda)
    (n,d) = size(X)
    w = zeros(d,1)

    funObj(w) = logisticObjL2(w,X,y,lambda)
    w = findMin(funObj,w,derivativeCheck=true)

    predict(Xhat) = sign.(Xhat*w)
    return LinearModel(predict,w)
end
\end{verbatim}
Using $\lambda = 1$:
Trainig error is increased to 0.002.
Validation error is decreased to 0.074.
Number of features and gradient descent iterations are the same (101 and 30 respectively).

\subsection{L1-Regularization}

\begin{verbatim}
function logisticObjL1(w,X,y,lambda)
    yXw = y.*(X*w)
    f = sum(log.(1 + exp.(-yXw))) + lambda*sum(abs.(w))
    g = -X'*(y./(1+exp.(yXw))) + lambda
    return (f,g)
end

function logRegL1(X,y,lambda)
    (n,d) = size(X)
    w = zeros(d,1)

    funObj(w) = logisticObjL1(w,X,y,lambda)
    w = findMinL1(funObj,w,lambda)

    predict(Xhat) = sign.(Xhat*w)
    return LinearModel(predict,w)
end
\end{verbatim}
 Using $\lambda = 1$: Training error is 0.002, validation error is 0.056, number of features selected is 72.


\subsection{L0-Regularization}

Code snippit: (please refer to \texttt{logReg.jl} for the whole function)
\begin{verbatim}
for j in setdiff(1:d,S)
    # Fit the model with 'j' added to the feature set 'S'
    # then compute the score and update 'minScore' and 'minS'
    Sj = [S;j]
    Xs = X[:,Sj]

    # PUT YOUR CODE HERE
    ds = size(Xs, 2)
    ws = zeros(ds, 1)

    # fit model
    funObj(ws) = logisticObj(ws,Xs,y)
    ws = findMin(funObj,ws,verbose=false)

    # compute score
    yXw = y.*(Xs*ws)
    score = sum(log.(1 + exp.(-yXw))) + lambda*ds

    # update min
    if score < minScore
        minScore = score
        minS = Sj
    end
end
\end{verbatim}

Useing $\lambda = 1$, the training error is 0.0, the validation error is 0.018, and the number of features selected is 24 (and they're all prime numbers, as expected).

\section{Very-Short Answer Questions}

\enum{
\item Using validation error to choose the number of features often results in overfitting, i.e. too many features are selected, since selecting more features isn't penalized and it usually means a better fit. We use a score BIC instead to make sure selecting too many features is penalized in proportion to the number of samples we have so that our classifier returns a reasonable subset of features.
\item Exhaustively searching through all the subsets takes exponential time, which is very expensive, so we go for a greedy approach, i.e. forward selection, that only takes polynomial time and produces a result that may not be optimal but would be good enough.
\item A small $\lambda$ means most of the result is determined by the objective function value, so it would result in a relatively small train error and large test error. A large $\lambda$ reduces overfitting as it places more weight on the penalization for feature selection, so it results in a larger train error but smaller test error.
\item For L1: L1 comes with feature selection. For L2: L2 has a closed-form unique solution (easier to solve)
\item Run the feature selection method on the bootstrap samples of the training data and take the intersection of the features selected in all bootstrap samples.
\item 'Being too right' is penalized significantly by least squares. E.g. if the labels are 1 and -1, $y_i$ is 1 and the classifier produces 100, which is correct, the error would be higher than if the classifer produces an incorrect result that's closer to the label, e.g. -2.
\item The SVM classifier maximizes the margin of the line so that changes to data points closer to the line would be likely to be classified correctly. The perceptron algorithm doesn't do that.
\item Polynomial basis increases exponentially with the number of features, so training and predicting data with a lot of features become very expensive very fast. We use the polynomial kernel to solve this as it allows us to compute $y$ wihtout the $Z$ matrix, using just $X$ so we won't need to store and compute all the term interactions needed in $Z$.
\item Which of the following methods produce linear classifiers? (a) binary least squares as in Question 3, (b) the perceptron algorithm, (c) SVMs, and (d) logistic regression. All of them. (TODO)
}

\end{document}
