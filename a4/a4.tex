\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{listings} % For displaying code
\usepackage{algorithm2e} % pseudo-code

% Answers
\def\ans#1{\par\gre{Answer: #1}}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{a4f/#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{a4f/#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}


\begin{document}

\title{CPSC 340 Assignment 5 (due November 17 ATE)}
\author{}
\date{}
\maketitle
\vspace{-4em}

Name: Hang Yee Wong\\
CS ID: r9i0b

Name: Joanne Chen\\
CS ID: r0a9

\section{Multi-Class Logistic}

\subsection{Softmax Classification}

\enum{
\item The data has 2 features and 3 possible labels, and in multi-class classification we need a set of weights for each possible label.
\item $\hat{x} W =
\begin{bmatrix}
1 & 4 & 2
\end{bmatrix}$. So we select the label that corresponds to the entry with the highest value, and in this case it is label 2.
}

\subsection{Softmax Loss}

$$f_{W_{jc}}(W) = \sum_{i=1}^n \left[-I(y_i = c)x_{ij} + x_{ij}\exp(w_c^Tx_i)/\left(\sum_{c' = 1}^k \exp(w_{c'}^Tx_i)\right)\right]$$
$$\to f_{W_{jc}}(W) = \sum_{i=1}^n \left[-I(y_i = c)x_{ij} + x_{ij}p(c|W,x_i)\right]$$
$$\to f_{W_{jc}}(W) = \sum_{i=1}^n x_{ij}(p(c|W,x_i)-I(y_i = c))$$


\subsection{Softmax Classifier}

Code:
\begin{verbatim}
function softmaxObj(w,X,y,k)
    (n,d) = size(X)
    W = reshape(w,d,k)
    f = 0
    g = zeros(d,k)

    partial = function(j,c)
            total = 0
            for i in 1:n
                    term1 = y[i] == c ? 1 : 0
                    term2 = exp(W[:,c]'*X[i,:]) / sum(exp.(X[i,:]'*W))
                    total += X[i,j]*(term2-term1)
            end
            return total
    end

    for i in 1:n
            f += -W[:,y[i]]'*X[i,:] + log(sum(exp.(X[i,:]'*W)))
    end

    for j in 1:d
            for c in 1:k
                    g[j,c] = partial(j,c)
            end
    end

    return (f,reshape(g,d*k,1))
end

function softmaxClassifier(X,y)
    (n,d) = size(X)
    k = maximum(y)

    W = zeros(d,k)
    Wp = reshape(W,d*k,1)

    funObj(w) = softmaxObj(w,X,y,k)

    Wp = findMin(funObj,Wp,derivativeCheck=true,verbose=false)
    W = reshape(Wp,d,k);
    @show(W)

    predict(Xhat) = mapslices(indmax,Xhat*W,2)

    return LinearModel(predict,W)
end
\end{verbatim}

Using softmax, the new train error is 0.004 and the validation error is 0.026.

\subsection{Cost of Multinomial Logistic Regression}

Assuming that we have
\items{
\item $n$ training examples.
\item $d$ features.
\item $k$ classes.
\item $t$ testing examples.
\item $T$ iterations of gradient descent for training.
}
\enum{
\item Computing the derivative of the objective function with respect to one entry in $W$ takes $n(d + dk)$ time. We need to take the derivatives with respect to all entries in $W$ to get the gradient, so computing the gradient takes $dk(nd + ndk)$ time. Using gradient descent we compute the gradient $T$ times so the total cost is $T(knd^2 + nk^2d^2)$, which is in $O(Tnk^2d^2)$
\item To predict, we must compute $XW$, which takes $tdk$ time, and then we find the max of each training example, which takes $tk$, so the total is $tdk + tk$, which is in $O(tdk)$
}



\section{MAP Estimation}

\enum{
\item $-\sum_{i=1}^n\log   \frac 1 2 \exp(-|w^Tx_i - y_i|)  -\sum_{j=1}^d\log \exp \frac{-\lambda(w_j -  w^0_j)^2}{2}$ \\
        $ =  \sum_{i=1}^n |w^Tx_i - y_i| +\sum_{j=1}^d  \frac{\lambda}{2}(w_j -  w^0_j)^2 + constant$\\
       $= \norm{Xw-y}_1 +  \frac{\lambda}{2} \norm{w -  w^0}^2_2 + constant$
\item  $-\sum_{i=1}^n\log  \frac{1}{\sqrt{2\sigma_i^2\pi}}\exp\left(-\frac{(w^Tx_i - y_i)^2}{2\sigma_i^2}\right) -\sum_{j=1}^d\log \frac{\lambda}{2}\exp(-\lambda|w_j|)  $\\
       $  =\sum_{i=1}^n( \frac{(w^Tx_i - y_i)^2}{2\sigma_i^2}+ constant) +  \sum_{j=1}^d (\lambda|w_j| +constant) $\\
       $ = \frac{1}{2}(Xw-y)^T \Sigma^{-1} (Xw-y) +\lambda\norm{w}_1+ constant $
\item  $-\sum_{i=1}^n\log  \frac{\exp(y_iw^Tx_i)\exp(-\exp(w^Tx_i))}{y_i!} -\sum_{i=1}^n \log  \frac{1}{\sqrt{2\sigma^2\pi}}\exp\left(-\frac{(w_j - 0)^2}{2\sigma^2}\right)$\\
        $= -\sum_{i=1}^n (y_iw^Tx_i) + \sum_{i=1}^n \exp(w^Tx_i) +\frac{1}{2\sigma}\norm{w}^2+ constant$
}


\section{Principal Component Analysis (2016)}

\subsection{PCA by Hand}

\enum{
\item $\mu$ is (0, 1), and the first principal component is $(\frac{1}{\sqrt2},\frac{1}{\sqrt2})$
\item $ z = (3-0)/ \sqrt2 +(3-1)/ \sqrt2 = 5/\sqrt2$ \\
        $ xhat = 5/\sqrt2 (\frac{1}{\sqrt2},\frac{1}{\sqrt2}) +(0,1) = (2.5,3.5)$\\
       $ reconstruction error = \sqrt{ (2.5-3)^2+(3.5-3)^2} = \frac{1}{\sqrt2} $
\item $ z = (3-0)/ \sqrt2 +(4-1)/ \sqrt2 = 6/\sqrt2$ \\
        $ xhat = 6/\sqrt2 (\frac{1}{\sqrt2},\frac{1}{\sqrt2}) +(0,1) = (3,4)$\\
       $ reconstruction error = \sqrt{ (3-3)^2+(4-4)^2} = 0 $
}


\subsection{Data Visualization}

\begin{enumerate}
\item  Graph: See Figure \ref{fig:a3_2_1_a}\\Code: 
\begin{verbatim}
# Load data
dataTable = readcsv("animals.csv")
X = float(real(dataTable[2:end,2:end]))
(n,d) = size(X)



function FrobeniusNorm(X)
     X0 = X.^2
     X1 = sum(X0)
     return X1
end

function VarianceRemain(ZW,X)

    return 1- vecnorm(ZW-X).^2/vecnorm(X).^2
end

# Standardize columns
include("misc.jl")
(X,mu,sigma) = standardizeCols(X)


include("PCA.jl")
k =13
model = PCA(X,k)

Z = model.compress(X)
ZW=model.expand(Z)

print(VarianceRemain(ZW,X))

# Plot matrix as image
using PyPlot
figure(1)
clf()
imshow(X)

# Show scatterplot of 2 random features

figure(2)
clf()
plot(Z[:,1],Z[:,2],".")
for i in (1:n)
    annotate(dataTable[i+1,1],
    xy=[Z[i,1],Z[i,2]],
    xycoords="data")
end

\end{verbatim}


\begin{figure}[h!]
    \includegraphics[width=50em,height=8.5cm]{q3_2_1_a.png}
    \caption{Question 3.2.1 Scatterplot using the first two features}
    \label{fig:a3_2_1_a}
\end{figure}
\item Running \texttt{indmax(abs.(model.W[1,:]))}, we get 12, corresponding to furry
\item Running \texttt{indmax(abs.(model.W[2,:]))}, we get 58, corresponding to grazer
\end{enumerate}


\vspace{10em}
\subsection{Data Compression}

\enum{
\item 30.19\% 
\item K = 5
\item K = 14
}


\section{Very-Short Answer Questions}


\enum{
\item Stochastic gradient methods use the gradient computed from one data point at each step instead of the gradient from all data points, so it's much faster when $n$ is large.
\item No, the minimum exists in a 'ball', the radius of which is proportional to the step size, so ideally we would decrease the step size as we draw closer to the optimal region to ensure convergence.
\item Multi-label: each data point may be assigned multiple labels. Mult-class: there are more than 2 labels.
\item In MLE, we're maximizing the probability of $y$ given $X, w$, but in MAP, we're maximizing the probability of $w$, given $X, y$.
\item No. Linear regression with one feature finds the line that has the minimal sum of the $y$ distances from the data points, whereas PCA with 2 features finds the line with the minimal sum of the perpendicular distances from teh data points.
\item No, because there are usually multiple ways to write the same vector space. For example, $\begin{bmatrix}1 & 0\end{bmatrix}$, $\begin{bmatrix}0 & 1\end{bmatrix}$ form the same vector space as $\begin{bmatrix}2 & 0\end{bmatrix}$, $\begin{bmatrix}0 & 2\end{bmatrix}$
\item 1) Linear regression with L1-regularization. 2) Non-negative least squares
\item No, because the minimizer computed from normal equations doesn't take the non-negative constraints into account and may be negative
}

\end{document}
