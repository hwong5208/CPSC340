\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{listings} % For displaying code
\usepackage{algorithm2e} % pseudo-code

% Answers
\def\ans#1{\par\gre{Answer: #1}}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{a3f/#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{a3f/#2}\end{center}}
\newcommand{\matCode}[1]{\lstinputlisting[language=Matlab]{a3f/#1.m}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}


\begin{document}

\title{CPSC 340 Assignment 3 (due November 3 ATE)}
\author{}
\date{}
\maketitle
\vspace{-4em}



\section{Convex Functions}

\enum{
\item $f(w) = \alpha w^2 - \beta w + \gamma$\\
         $f'(w) = 2\alpha w  - \beta $\\
          $f''(w) = 2\alpha   >0  $, so it's convex.
\item $f(w) = w\log(w) $ with $w > 0$\\ 
       $f'(w) = w(\frac{1}{w})+ \log(w) $\\
       $f'(w) = 1+ \log(w) $ \\
       $f''(w) = \frac{1}{w}  >0 $, so it's convex.
\item $f(w) = \norm{Xw-y}^2 + \lambda\norm{w}_1$ \\
        we have $ \norm{Xw-y}^2$ is a convex function because it is norm.\\
       And also, $\lambda\norm{w}_1$  is a convex function because it is norm and $\lambda \geq 0$.\\ $f(w)$ is the sum of two convex functions so it is also convex. 
\item Let $g(w) = -y_iw^Tx_i$, $g(w)$ is convex because it is linear.\\
      Let  $g(z) = \log(1+\exp(Z))$\\
        $g'(z) = \frac{exp(z)}{1+exp(z)}$\\
         $g'(z) = \frac{1}{1+exp(-z)}$\\
        $g''(z) =  \frac{exp(-z)}{(1+exp(-z))^2}   > 0 $. So $g(z)$ is also convex, and the composition $g(h(w)) = log(1+exp(-y_iw^Tx_i))$ is also convex. $f(w)$ is the sum of $g(h(w))$ so it is also convex.
\item $f(w,w_0) = \sum_{i=1}^N\max\{0,w_0 - w^Tx_i\} - w_0 + \frac{\lambda}{2}\norm{w}_2^2$\\
        $\max\{0,w_0 - w^Tx_i\} $ is a max of convex functions, so it is also a convex function. $\sum_{i=1}^N\max\{0,w_0 - w^Tx_i\} - w_0  $ is a composition of a convex function and linear function, so it is also a convex function. $\norm{w}_2^2$ is a norm and $\lambda \geq 0$, so $ \frac{\lambda}{2}\norm{w}_2^2$\ is convex function.
Therefore, $f(w)$, the sum of all these convex functions, is a convex functon.
}


\section{Gaussian RBFs and Regularization}

\subsection{Regularization}

\begin{verbatim}
function leastSquaresRBF(X,y,sigma,lambda)
    (n,d) = size(X)

    Z = rbf(X,X,sigma)


    w = (Z'*Z+lambda*eye(n))\(Z'*y)

    predict(Xhat) = rbf(Xhat,X,sigma)*w

    return LinearModel(predict,w)
end
\end{verbatim}

After adding regularization with $\lambda = 10^{-12}$, the choice of $\sigma$ becomes a bit more stable and it always chooses a relatively small value for $\sigma$, resulting in a smaller test error. (After running a few times, the $\sigma =$ values it picked most often were 1 and 4, and occasiaonally 8, whereas originally the $\sigma$ value could go up to 32 and 64).

\subsection{Cross-Validation}

\begin{verbatim}
    minErr = Inf
    nsplits = 10
    bestSigma = []
    for sigma in 2.0.^(-15:15)
            validError =0
            for splits in 1:nsplits
                    validStart = Int64(1+(n/nsplits)*(splits-1)) # Start of validation indices
                    validEnd = Int64((n/nsplits)*splits) # End of validation incides
                    validNdx = perm[validStart:validEnd] # Indices of validation examples
                    trainNdx = perm[setdiff(1:n,validStart:validEnd)]
                    Xtrain = X[trainNdx,:]
                    ytrain = y[trainNdx]
                    Xvalid = X[validNdx,:]
                    yvalid = y[validNdx]
    
    
                    # Train on the training set
                    model = leastSquaresRBF(Xtrain,ytrain,sigma,1/10^(12))
    
                    # Compute the error on the validation set
                    yhat = model.predict(Xvalid)
                    validError = sum((yhat - yvalid).^2)/(n/nsplits)
            end
            @printf("With sigma = %.3f, validError = %.2f\n",sigma,validError)
    
            # Keep track of the lowest validation error
            if validError < minErr
                    minErr = validError
                    bestSigma = sigma
            end
    
    end
\end{verbatim}

Using cross-validation, we get $\sigma = 1$, which yields the lowest test error, most of the time. (We got nine 1's and one 0.5 after running it 10 times)


\subsection{Cost of Non-Parametric Bases}

When dealing with larger datasets, an important issue is the dependence of the computational cost on the number of training examples $n$ and the number of features $d$. 
\blu{
\enum{
\item What is the cost in big-O notation of training a linear regression model with Gaussian RBFs on $n$ training examples with $d$ features (for fixed $\sigma$ and $\lambda$)? 
\item What is the cost of classifying $t$ new examples with this model? 
\item When is it cheaper to train using Gaussian RBFs than using the basic linear basis? 
\item When is it cheaper to test using Gaussian RBFs than using the basic linear basis?
}}

\enum{
\item  The cost to construct the $Z$ matrix is $O(n^2d)$, the cost to construct the linear system is $O(n^3)$ and the cost of computing the solution $w$ is $O(n^3)$, so the training cost of Gaussian RBF is O($n^2d+n^3$).
\item For each test example, we need to 1) compute the norm of between it and every original data point, which takes $O(nd)$ time, so the total is $O(tnd)$.
\item  The cost to construct the Z matrix is $O(nd)$, the cost to construct the linear system is $O(nd^2)$, and the cost to solve the linear equations is $O(d^3)$, so basic linear basis takes $O(nd^2 + d^3)$ time, so RBF is cheaper when $ n < d$.
 \item  Basic linear basis takes $O(td)$ to test, so it's always cheaper than RBF.
}



\section{Logistic Regression with Sparse Regularization}

\subsection{Logistic Regression}

Using least squares the train error, validation error, and number of features are: 0.038, 0.106, 101 respectively. Using logistic regression the train error, validation error, and number of features are: 0.0, 0.082, 101 respectively. From the results, we can see that both the train and validation errors go down when we switch to logistic regression, but the number of features selected remains the same.

\subsection{L2-Regularization}

\begin{verbatim}
function logisticObjL2(w,X,y,lambda)
    yXw = y.*(X*w)
    f = sum(log.(1 + exp.(-yXw))) + (lambda/2)*sum(w.*w)
    g = -X'*(y./(1+exp.(yXw))) + lambda*w
    return (f,g)
end

function logRegL2(X,y,lambda)
    (n,d) = size(X)
    w = zeros(d,1)

    funObj(w) = logisticObjL2(w,X,y,lambda)
    w = findMin(funObj,w,derivativeCheck=true)

    predict(Xhat) = sign.(Xhat*w)
    return LinearModel(predict,w)
end
\end{verbatim}
Using $\lambda = 1$:
Trainig error is increased to 0.002.
Validation error is decreased to 0.074.
Number of features and gradient descent iterations are the same (101 and 30 respectively).

\subsection{L1-Regularization}

\begin{verbatim}
function logisticObjL1(w,X,y,lambda)
    yXw = y.*(X*w)
    f = sum(log.(1 + exp.(-yXw))) + lambda*sum(abs.(w))
    g = -X'*(y./(1+exp.(yXw))) + lambda
    return (f,g)
end

function logRegL1(X,y,lambda)
    (n,d) = size(X)
    w = zeros(d,1)

    funObj(w) = logisticObjL1(w,X,y,lambda)
    w = findMinL1(funObj,w,lambda)

    predict(Xhat) = sign.(Xhat*w)
    return LinearModel(predict,w)
end
\end{verbatim}
 Using $\lambda = 1$: Training error is 0.002, validation error is 0.056, number of features selected is 72.


\subsection{L0-Regularization}

Code snippit: (please refer to \texttt{logReg.jl} for the whole function)
\begin{verbatim}
for j in setdiff(1:d,S)
    # Fit the model with 'j' added to the feature set 'S'
    # then compute the score and update 'minScore' and 'minS'
    Sj = [S;j]
    Xs = X[:,Sj]

    # PUT YOUR CODE HERE
    ds = size(Xs, 2)
    ws = zeros(ds, 1)

    # fit model
    funObj(ws) = logisticObj(ws,Xs,y)
    ws = findMin(funObj,ws,verbose=false)

    # compute score
    yXw = y.*(Xs*ws)
    score = sum(log.(1 + exp.(-yXw))) + lambda*ds

    # update min
    if score < minScore
        minScore = score
        minS = Sj
    end
end
\end{verbatim}

Useing $\lambda = 1$, the training error is 0.0, the validation error is 0.018, and the number of features selected is 24 (and they're all prime numbers, as expected).

\section{Very-Short Answer Questions}

\enum{
\item Using validation error to choose the number of features often results in overfitting, i.e. too many features are selected, since selecting more features isn't penalized and it usually means a better fit. We use a score BIC instead to make sure selecting too many features is penalized in proportion to the number of samples we have so that our classifier returns a reasonable subset of features.
\item Exhaustively searching through all the subsets takes exponential time, which is very expensive, so we go for a greedy approach, i.e. forward selection, that only takes polynomial time and produces a result that may not be optimal but would be good enough.
\item A small $\lambda$ means most of the result is determined by the objective function value, so it would result in a relatively small train error and large test error. A large $\lambda$ reduces overfitting as it places more weight on the penalization for feature selection, so it results in a larger train error but smaller test error.
\item For L1: L1 comes with feature selection. For L2: L2 has a closed-form unique solution (easier to solve)
\item Run the feature selection method on the bootstrap samples of the training data and take the intersection of the features selected in all bootstrap samples.
\item 'Being too right' is penalized significantly by least squares. E.g. if the labels are 1 and -1, $y_i$ is 1 and the classifier produces 100, which is correct, the error would be higher than if the classifer produces an incorrect result that's closer to the label, e.g. -2.
\item The SVM classifier maximizes the margin of the line so that changes to data points closer to the line would be likely to be classified correctly. The perceptron algorithm doesn't do that.
\item Polynomial basis increases exponentially with the number of features, so training and predicting data with a lot of features become very expensive very fast. We use the polynomial kernel to solve this as it allows us to compute $y$ wihtout the $Z$ matrix, using just $X$ so we won't need to store and compute all the term interactions needed in $Z$.
\item They all have linear prediction functions and they all 'split' the data points into two groups (in the transformed space), so they're all linear classifiers.
}

\end{document}
